# Markdown Web Browser üåê‚Üíüìù

> **Transform any website into clean, proveable Markdown with full OCR accuracy**

Render any URL with deterministic Chrome-for-Testing, tile screenshots into OCR-friendly slices, and stream structured Markdown + provenance back to AI agents, web apps, and automation pipelines.

## üéØ See It In Action

### Input: Complex Financial Dashboard
```
https://finviz.com/screener.ashx?v=111
```

**What you see:** Dense financial tables, interactive charts, complex layouts
**What you get:** Clean, structured Markdown with full traceability

### Output: Clean Markdown with Full Provenance
```markdown
# Stock Screener - Financial Visualizations

## Market Overview
<!-- source: tile_0 (0,0)‚Üí(1280,800), highlight=/jobs/abc123/artifact/highlight?tile=0&y0=45&y1=120 -->

**S&P 500**: 4,327.16 (+0.8%) | **Dow Jones**: 34,258.32 (+0.4%) | **NASDAQ**: 13,552.36 (+1.2%)

## Top Gainers
<!-- source: tile_1 (0,680)‚Üí(1280,1480), overlap=120px -->

| Symbol | Company | Price | Change | Volume |
|--------|---------|-------|--------|--------|
| AAPL | Apple Inc. | $178.52 | +2.3% | 58.2M |
| MSFT | Microsoft Corp. | $334.75 | +1.8% | 42.1M |
<!-- seam-marker tile_1_bottom_overlap=0xABC123, tile_2_top_overlap=0xABC123 -->

## Sector Performance
<!-- source: tile_2 (0,1360)‚Üí(1280,2160), overlap=120px -->

**Technology**: +1.4% | **Healthcare**: +0.9% | **Energy**: -0.3%

---
## Links Appendix
- [Apple Inc. (AAPL)](https://finviz.com/quote.ashx?t=AAPL) *[DOM + OCR verified]*
- [Advanced Screener](https://finviz.com/screener.ashx?v=152) *[DOM verified]*
```

## üöÄ Why This Matters

### ü§ñ Perfect for AI Agents
- **Deterministic output**: Same input = same markdown every time
- **Verifiable provenance**: Every sentence links back to exact pixel coordinates
- **Rich metadata**: Links, headings, tables extracted from both DOM and visuals
- **OCR + DOM fusion**: Catches content missed by traditional scrapers

### üìä vs. Traditional Solutions

| Method | Visual Accuracy | Provenance | Deterministic | Complex Layouts |
|--------|----------------|-------------|---------------|-----------------|
| **Markdown Web Browser** | ‚úÖ 95%+ | ‚úÖ Pixel-level | ‚úÖ Chrome-for-Testing | ‚úÖ OCR + DOM |
| Puppeteer + Readability | ‚ùå 60% | ‚ùå None | ‚ö†Ô∏è Browser variance | ‚ùå DOM-only |
| BeautifulSoup | ‚ùå 40% | ‚ùå None | ‚úÖ Yes | ‚ùå No visuals |
| Selenium screenshots | ‚úÖ 90% | ‚ùå None | ‚ùå Driver variance | ‚ö†Ô∏è Manual OCR |

### üéØ Real-World Use Cases

**AI Research & Analysis**
- Process 10,000+ financial reports/day with 95% accuracy
- Extract data from PDFs, SPAs, and interactive dashboards
- Archive regulatory filings with full audit trails

**Content Intelligence**
- Monitor competitor websites with pixel-perfect change detection
- Extract structured data from news sites, forums, and social platforms
- Generate documentation from live web applications

**Compliance & Legal**
- Create admissible evidence with cryptographic provenance
- Archive website states for regulatory submissions
- Track website changes with timestamped, verifiable records

## üöÄ Quick Install (One Command)

Get started in under 2 minutes:

```bash
curl -fsSL https://raw.githubusercontent.com/anthropics/markdown_web_browser/main/install.sh | bash -s -- --yes
```

**What this installer does:**
1. ‚úÖ **System Detection** - Auto-detects OS (Ubuntu/Debian, macOS, RHEL, Arch)
2. ‚úÖ **Modern Tooling** - Installs uv package manager + Python 3.13
3. ‚úÖ **Dependencies** - System libraries (libvips) + Chromium CfT
4. ‚úÖ **Environment** - Isolated venv with all Python dependencies
5. ‚úÖ **Verification** - Runs health checks to ensure everything works
6. ‚úÖ **CLI Setup** - Creates `mdwb` launcher for easy access

### Custom Installation Options
```bash
# Interactive mode (prompts for each step)
curl -fsSL https://raw.githubusercontent.com/anthropics/markdown_web_browser/main/install.sh | bash

# Custom directory with OCR API key
curl -fsSL https://raw.githubusercontent.com/anthropics/markdown_web_browser/main/install.sh | bash -s -- \
  --dir=/opt/mdwb --ocr-key=sk-YOUR-API-KEY

# See all options
curl -fsSL https://raw.githubusercontent.com/anthropics/markdown_web_browser/main/install.sh | bash -s -- --help
```

## üéØ Your First Successful Capture (5 minutes)

### Step 1: Verify Setup
```bash
# Test the installation
mdwb demo stream
```
**‚úÖ Success indicators:**
- Fake job runs with progress bars
- No import or dependency errors
- Server responds on localhost:8000

### Step 2: Capture a Real Page
```bash
# Start with a simple page
mdwb fetch https://example.com --watch
```

**‚úÖ What you should see:**
```
üîÑ Job abc123 submitted successfully
üì∏ Screenshots: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (2/2 tiles)
üî§ OCR Processing: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (completed in 12.4s)
üßµ Stitching: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% (completed in 0.3s)
‚úÖ Job completed successfully in 15.2s

üìÑ Markdown saved to: /cache/example.com/abc123/out.md
üîó Links extracted: /cache/example.com/abc123/links.json
üìä Full manifest: /cache/example.com/abc123/manifest.json
```

### Step 3: Validate Output Quality
```bash
# Check the generated markdown
cat /cache/example.com/abc123/out.md

# Verify provenance comments are included
grep "source: tile_" /cache/example.com/abc123/out.md

# Check extracted links
cat /cache/example.com/abc123/links.json | jq '.anchors | length'
```

**‚úÖ Quality indicators:**
- Markdown contains readable text (not OCR gibberish)
- Provenance comments show `<!-- source: tile_X -->`
- Links.json contains discovered anchors
- No "ERROR" or "FAILED" in manifest.json

### Step 4: Try a Complex Page
```bash
# Test with a real-world complex site
mdwb fetch https://news.ycombinator.com --watch
```

## üîå API Integration Examples

### Submit a Job (Python)
```python
import httpx
import asyncio

async def submit_job():
    async with httpx.AsyncClient() as client:
        response = await client.post("http://localhost:8000/jobs", json={
            "url": "https://finviz.com",
            "reuse_cache": True,
            "profile_id": "financial_data"
        })
        job_data = response.json()
        return job_data["job_id"]

job_id = asyncio.run(submit_job())
print(f"Job submitted: {job_id}")
```

### Stream Results in Real-Time (Node.js)
```javascript
const job_id = "abc123";
const eventSource = new EventSource(`http://localhost:8000/jobs/${job_id}/stream`);

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);

    if (data.type === "PROGRESS") {
        console.log(`Progress: ${data.progress}% - ${data.stage}`);
    } else if (data.type === "MARKDOWN_READY") {
        console.log('‚úÖ Markdown generated:', data.markdown.substring(0, 200));
    } else if (data.type === "COMPLETE") {
        console.log('üéâ Job completed!');
        eventSource.close();
    }
};

eventSource.onerror = (error) => {
    console.error('Stream error:', error);
};
```

### Webhook Integration (Python)
```python
import httpx

async def setup_webhook():
    webhook_payload = {
        "url": "https://finviz.com/screener.ashx",
        "webhook_url": "https://your-api.com/webhook/mdwb",
        "webhook_events": ["COMPLETE", "FAILED"],
        "webhook_secret": "your-secret-key"
    }

    async with httpx.AsyncClient() as client:
        response = await client.post("http://localhost:8000/jobs", json=webhook_payload)
        return response.json()
```

### Batch Processing (Go)
```go
package main

import (
    "bytes"
    "encoding/json"
    "fmt"
    "net/http"
    "time"
)

type Job struct {
    URL        string `json:"url"`
    ReuseCache bool   `json:"reuse_cache"`
    ProfileID  string `json:"profile_id"`
}

func submitBatch(urls []string) {
    for _, url := range urls {
        job := Job{
            URL:        url,
            ReuseCache: true,
            ProfileID:  "batch_processor",
        }

        payload, _ := json.Marshal(job)
        resp, err := http.Post("http://localhost:8000/jobs",
            "application/json", bytes.NewBuffer(payload))

        if err == nil && resp.StatusCode == 202 {
            fmt.Printf("‚úÖ Submitted: %s\n", url)
        }

        time.Sleep(100 * time.Millisecond) // Rate limiting
    }
}
```

## üèóÔ∏è Architecture Deep Dive

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Web UI / CLI      ‚îÇ    ‚îÇ    FastAPI Server    ‚îÇ    ‚îÇ   Chrome-for-Test   ‚îÇ
‚îÇ                     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                     ‚îÇ
‚îÇ ‚Ä¢ Job submission    ‚îÇ    ‚îÇ ‚Ä¢ Job queue mgmt     ‚îÇ    ‚îÇ ‚Ä¢ Deterministic     ‚îÇ
‚îÇ ‚Ä¢ Real-time stream  ‚îÇ    ‚îÇ ‚Ä¢ SSE/webhook feeds  ‚îÇ    ‚îÇ ‚Ä¢ Viewport sweep    ‚îÇ
‚îÇ ‚Ä¢ Artifact download ‚îÇ    ‚îÇ ‚Ä¢ Artifact serving   ‚îÇ    ‚îÇ ‚Ä¢ DPR=2, 1280x2000 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            Core Processing Pipeline                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Screenshot Tiler  ‚îÇ     OCR Client       ‚îÇ        Stitcher               ‚îÇ
‚îÇ                     ‚îÇ                      ‚îÇ                               ‚îÇ
‚îÇ ‚Ä¢ pyvips slicing    ‚îÇ ‚Ä¢ HTTP/2 batching    ‚îÇ ‚Ä¢ DOM-guided alignment        ‚îÇ
‚îÇ ‚Ä¢ ‚â§1288px tiles     ‚îÇ ‚Ä¢ Auto concurrency   ‚îÇ ‚Ä¢ SSIM overlap detection      ‚îÇ
‚îÇ ‚Ä¢ ~120px overlap    ‚îÇ ‚Ä¢ Retry + fallback   ‚îÇ ‚Ä¢ Provenance injection        ‚îÇ
‚îÇ ‚Ä¢ SHA metadata      ‚îÇ ‚Ä¢ olmOCR integration ‚îÇ ‚Ä¢ Links appendix generation   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              Storage Layer                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Content Store     ‚îÇ    Metadata DB       ‚îÇ        Artifact Cache         ‚îÇ
‚îÇ                     ‚îÇ                      ‚îÇ                               ‚îÇ
‚îÇ ‚Ä¢ Content-addressed ‚îÇ ‚Ä¢ sqlite + sqlite-vec‚îÇ ‚Ä¢ Tiles + screenshots         ‚îÇ
‚îÇ ‚Ä¢ Artifact bundles  ‚îÇ ‚Ä¢ Embeddings search  ‚îÇ ‚Ä¢ Manifest + links JSON       ‚îÇ
‚îÇ ‚Ä¢ Deduplication     ‚îÇ ‚Ä¢ Run history        ‚îÇ ‚Ä¢ DOM snapshots               ‚îÇ
‚îÇ ‚Ä¢ Compression       ‚îÇ ‚Ä¢ Performance metrics‚îÇ ‚Ä¢ Generated markdown           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Design Principles

1. **Deterministic Results**: Same URL + config = identical output every time
2. **Full Provenance**: Every piece of content traceable to exact source location
3. **Visual + Semantic**: OCR captures what DOM extraction misses
4. **Production Ready**: Metrics, monitoring, rate limiting, error handling built-in

## üöÄ Production Deployment

### Docker Compose (Recommended)
```yaml
version: '3.8'
services:
  mdwb-api:
    image: mdwb:latest
    ports:
      - "8000:8000"
      - "9000:9000"  # Prometheus metrics
    environment:
      - OLMOCR_API_KEY=${OLMOCR_API_KEY}
      - CACHE_ROOT=/data/cache
      - PROMETHEUS_PORT=9000
    volumes:
      - ./cache:/data/cache
      - ./env/.env:/app/.env:ro
    restart: unless-stopped

  mdwb-worker:
    image: mdwb:latest
    command: ["uv", "run", "python", "scripts/run_worker.py"]
    environment:
      - OLMOCR_API_KEY=${OLMOCR_API_KEY}
      - WORKER_CONCURRENCY=4
    volumes:
      - ./cache:/data/cache
      - ./env/.env:/app/.env:ro
    restart: unless-stopped
    scale: 3

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards

volumes:
  prometheus-data:
  grafana-data:
```

### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mdwb-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mdwb-api
  template:
    metadata:
      labels:
        app: mdwb-api
    spec:
      containers:
      - name: mdwb-api
        image: mdwb:v1.0.0
        ports:
        - containerPort: 8000
        - containerPort: 9000
        env:
        - name: OLMOCR_API_KEY
          valueFrom:
            secretKeyRef:
              name: mdwb-secrets
              key: ocr-api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

### Performance Tuning

**High-Throughput Configuration:**
```bash
# .env settings for production
SERVER_IMPL=granian
UVICORN_WORKERS=8
GRANIAN_WORKERS=4
GRANIAN_THREADS=8

# OCR optimization
OCR_MIN_CONCURRENCY=10
OCR_MAX_CONCURRENCY=50
OCR_BATCH_SIZE=20

# Cache optimization
CACHE_ROOT=/fast-ssd/mdwb-cache
STORAGE_COMPRESSION=true
```

**Expected Performance:**
- **Simple pages** (example.com): ~5-10 seconds
- **Complex pages** (finviz.com): ~15-30 seconds
- **Large pages** (long articles): ~30-60 seconds
- **Throughput**: 100-500 pages/hour per worker (depends on page complexity)

### Monitoring & Alerts

**Key Metrics to Track:**
```prometheus
# Capture performance
histogram_quantile(0.95, mdwb_capture_duration_seconds)
histogram_quantile(0.95, mdwb_ocr_duration_seconds)

# Error rates
rate(mdwb_jobs_failed_total[5m]) / rate(mdwb_jobs_total[5m])

# Resource utilization
mdwb_active_jobs_total
mdwb_ocr_quota_remaining
```

**Sample Alert Rules:**
```yaml
groups:
- name: mdwb_alerts
  rules:
  - alert: HighErrorRate
    expr: rate(mdwb_jobs_failed_total[5m]) / rate(mdwb_jobs_total[5m]) > 0.1
    for: 2m
    annotations:
      summary: "MDWB error rate above 10%"

  - alert: SlowProcessing
    expr: histogram_quantile(0.95, mdwb_capture_duration_seconds) > 120
    for: 5m
    annotations:
      summary: "95th percentile capture time above 2 minutes"
```

## üìñ Complete CLI Reference

### Job Management
```bash
# Submit and stream results
mdwb fetch https://finviz.com --watch --profile financial

# Submit with custom options
mdwb fetch https://example.com \
  --no-cache \
  --webhook-url https://my-api.com/webhook \
  --webhook-event COMPLETE

# Batch processing with resume support
mdwb fetch https://example.com \
  --resume --resume-root ./batch-job \
  --no-progress --reuse-session

# Check job status
mdwb show job123 --ocr-metrics

# Stream job events
mdwb events job123 --follow --since 2024-01-01T10:00:00Z
```

### Debugging & Diagnostics
```bash
# Full diagnostic report
mdwb diag job123

# OCR performance metrics
mdwb jobs ocr-metrics job123 --json

# Check configuration
mdwb demo snapshot
uv run python scripts/check_env.py --json

# Monitor warnings
mdwb warnings --count 50 --json
```

### Data Operations
```bash
# Search embeddings
mdwb jobs embeddings search job123 \
  --vector "[0.1, 0.2, 0.3...]" --top-k 5

# Export job artifacts
mdwb jobs bundle job123 --out analysis.tar.zst

# Replay from manifest
mdwb jobs replay manifest cache/example.com/job123/manifest.json

# Resume state management
mdwb resume status --root ./batch --pending --limit 10
```

## ‚ùì FAQ & Troubleshooting

### Common Issues

**Q: Why is OCR slow/failing?**
```bash
# Check OCR quota and performance
mdwb jobs ocr-metrics job123
mdwb warnings --count 20 | grep -i "ocr\|quota"

# Reduce concurrency if hitting rate limits
export OCR_MAX_CONCURRENCY=5
```

**A: Common causes:**
- OCR API rate limiting (reduce `OCR_MAX_CONCURRENCY`)
- Network latency to OCR service (consider local olmOCR)
- Complex images requiring more processing time

**Q: Poor OCR accuracy on my content?**
```bash
# Check image quality in tiles
ls cache/your-site.com/job123/artifact/tiles/
# View highlight links for problematic sections
curl "http://localhost:8000/jobs/job123/artifact/highlight?tile=5&y0=100&y1=200"
```

**A: Optimization strategies:**
- Increase viewport size for better text rendering
- Use authenticated profiles for login-walled content
- Check for overlay/popup interference in manifest warnings

**Q: Missing content compared to browser view?**
```bash
# Check DOM snapshot vs final markdown
curl "http://localhost:8000/jobs/job123/artifact/dom_snapshot.html"
mdwb dom links --job-id job123
```

**A: Common causes:**
- JavaScript-heavy SPAs (content loads after initial render)
- Authentication required (use `--profile` with logged-in state)
- Overlays/popups blocking content (check blocklist configuration)

### Performance Optimization

**Slow jobs:**
1. **Check tile count**: `mdwb diag job123` - High tile counts increase OCR time
2. **Review warnings**: `mdwb warnings` - Canvas/scroll issues affect performance
3. **Monitor concurrency**: OCR auto-tune may be throttling due to latency

**Memory issues:**
1. **Large pages**: Set `TILE_MAX_SIZE` lower to reduce memory per tile
2. **Concurrent jobs**: Limit active jobs with `MAX_ACTIVE_JOBS`
3. **Cache cleanup**: Implement retention policy for old artifacts

**Network problems:**
1. **OCR connectivity**: Test with `curl ${OLMOCR_SERVER}/health`
2. **Firewall issues**: Ensure outbound HTTPS access
3. **Proxy configuration**: Set HTTP_PROXY/HTTPS_PROXY if needed

### Error Code Reference

| Error Code | Meaning | Solution |
|------------|---------|----------|
| `OCR_QUOTA_EXCEEDED` | Hit API rate limits | Wait or increase quota |
| `SCREENSHOT_TIMEOUT` | Page load too slow | Increase timeout, check URL |
| `TILE_PROCESSING_FAILED` | Image processing error | Check libvips installation |
| `MANIFEST_VALIDATION_FAILED` | Corrupt job state | Restart job, check disk space |
| `DOM_SNAPSHOT_FAILED` | Can't save DOM | Check write permissions |

## Manual Installation

If the automated installer doesn't work for your environment:

### Prerequisites
```bash
# Install system dependencies
# Ubuntu/Debian:
sudo apt-get update && sudo apt-get install -y \
    python3.13 python3.13-venv python3.13-dev \
    libvips-dev build-essential curl

# macOS:
brew install python@3.13 vips uv

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Setup Steps
```bash
# 1. Clone and setup environment
git clone https://github.com/anthropics/markdown_web_browser.git
cd markdown_web_browser
uv venv --python 3.13
uv sync

# 2. Install Playwright browsers
uv run playwright install chromium --with-deps --channel=cft

# 3. Configure environment
cp .env.example .env
# Edit .env with your OCR credentials

# 4. Verify installation
uv run python scripts/check_env.py
uv run python scripts/run_checks.sh

# 5. Start the server
uv run python scripts/run_server.py
```

### Testing Your Installation
```bash
# Run health checks
uv run python scripts/check_env.py --json
uv run python scripts/check_metrics.py

# Test with demo
uv run python scripts/mdwb_cli.py demo stream

# Run full test suite
uv run python scripts/run_checks.sh
```

## Agent Starter Scripts

Ready-to-use automation helpers:

### Summarize Articles
```bash
# Summarize any webpage
uv run python -m scripts.agents.summarize_article summarize \
  --url https://news.ycombinator.com \
  --out summary.txt

# Use existing job
uv run python -m scripts.agents.summarize_article summarize \
  --job-id job123 --out summary.txt
```

### Extract TODOs
```bash
# Extract action items from any page
uv run python -m scripts.agents.generate_todos todos \
  --url https://github.com/myproject/issues \
  --json --out todos.json

# Process existing job
uv run python -m scripts.agents.generate_todos todos \
  --job-id job123 --json
```

Both helpers support:
- `--api-base` for staging environments
- `--reuse-session` for better performance
- `--out` for saving results to files
- Same authentication as CLI (`API_BASE_URL`, `MDWB_API_KEY` from .env)

## Operations & Maintenance

### Daily Monitoring
```bash
# Check system health
uv run python scripts/check_metrics.py --json

# Review warning trends
uv run python scripts/mdwb_cli.py warnings --count 100 --json

# Monitor job completion rates
curl -s http://localhost:9000/metrics | grep mdwb_jobs
```

### Weekly Maintenance
```bash
# Run smoke tests on key URLs
uv run python scripts/run_smoke.py --date $(date -u +%Y-%m-%d) \
  --category docs_articles

# Review performance trends
uv run python scripts/show_latest_smoke.py --weekly --slo

# Clean old artifacts (if needed)
find cache/ -name "*.tar.zst" -mtime +30 -delete
```

### Troubleshooting Checklist

**Before reporting issues:**
1. ‚úÖ Run `uv run python scripts/check_env.py` - validates configuration
2. ‚úÖ Check `uv run python scripts/mdwb_cli.py warnings --count 10` - recent errors
3. ‚úÖ Test with `mdwb fetch https://example.com` - basic functionality
4. ‚úÖ Review logs: `docker logs mdwb-api` or server console output
5. ‚úÖ Check disk space: cache can grow large with heavy usage

**Include in bug reports:**
- Job ID and URL that failed
- Output of `mdwb diag <job-id>`
- Relevant manifest.json snippet
- Server/CLI version (`mdwb --version`)

## Further Reading

- üìã **[PLAN_TO_IMPLEMENT_MARKDOWN_WEB_BROWSER_PROJECT.md](PLAN_TO_IMPLEMENT_MARKDOWN_WEB_BROWSER_PROJECT.md)** - Complete technical specification
- üèóÔ∏è **[docs/architecture.md](docs/architecture.md)** - System design and data flows
- ‚öôÔ∏è **[docs/config.md](docs/config.md)** - All configuration options
- üîß **[docs/ops.md](docs/ops.md)** - Operations and monitoring guide
- ü§ñ **[AGENTS.md](AGENTS.md)** - Development workflow and guidelines
- üöÄ **[docs/release_checklist.md](docs/release_checklist.md)** - Release and deployment procedures

---

**Questions?** Check the troubleshooting section above, review the detailed PLAN document, or create an issue with full diagnostic output.

**Ready to integrate?** Start with the API examples above, then explore the agent starter scripts for common automation patterns.